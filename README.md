# ESWT-Ne: Image Inpainting Using Two-Stage Adversarial Networks with Efficient Stripe window Transformer

Image inpainting has been research and made significant advances in recent years. However, it is still challenging to recover the corrupted image with both visually reasonable structure and the continuity of adjacent textures. Some state-of-the-art (SOTA) methods only tackle the textures while losing holistic structures due to the limited receptive fields of convolutional neural networks (CNNs). On the other hand, attention-based models can notice the global information for the structures, but they need the heavy computation for inference. To address these issues, we propose to use a rectangle window transformer to facilitate structure of the image inpainting. The proposed model easily acquires long-range dependencies through a powerful attention-based Transformer model. For textures, we propose a new color loss to progressively inpainting the color. The average color of our recovered images can be closer to reality. At last, we in order to enhance the recover effect, we additionally design a self-attention based on CNNbased attention sharing. Compared with other state-of-theart methods, we validate the effectiveness of our proposed model on various datasets.

<img src="https://i.imgur.com/LBIfNME.png" alt="https://i.imgur.com/LBIfNME.png" title="https://i.imgur.com/LBIfNME.png" width="1312" height="350">

# Environment
- Python 3.7.0
- pytorch
- opencv
- PIL  
- colorama

or see the requirements.txt

# How to try

## Download dataset (Places365、CelebA、ImageNet)
[Places365](http://Places365.csail.mit.edu/)  
[CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)  
[FFHQ](https://drive.google.com/drive/folders/1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP)
[Paris Street View] (Non-publicise, please ask the paper author)

## Set dataset path

Edit txt/xxx.txt (set path in config)
```python

data_path = './txt/train_path.txt'
mask_path = './txt/train_mask_path.txt'
val_path = './txt/val_path.txt'
val_mask_path = './val_mask_file/' # path
test_path: './txt/test_path.txt'
test_mask_1_60_path: './test_mask_1+10_file/' # path

```

txt example
```python

E:/Places365/data_256/00000001.jpg
E:/Places365/data_256/00000002.jpg
E:/Places365/data_256/00000003.jpg
E:/Places365/data_256/00000004.jpg
E:/Places365/data_256/00000005.jpg
 ⋮

```

You can refer to our example.txt in the txt path

## Preprocessing  
In this implementation, masks are automatically generated by ourself. stroke masks mixed randomly to generate proportion from 1% to 60%.

strokes (from left to right 20%-30% 30%-40% 40%-50% 50%-60%)
<img src="https://imgur.com/m3CStkN.png" alt="https://imgur.com/m3CStkN.png" title="https://imgur.com/m3CStkN.png" width="1000" height="200">

## Pretrained model
["Here"](https://drive.google.com/drive/u/4/folders/1QeLZc7_4TZVZ7awRQXNmgvGtPl6OGUAR)

## Run training
```python

python train.py (main setting data_path/mask_path/val_path/val_mask_path/batch_size/train_epoch)

```
1. set the config path ('./config/model_config.yml')
2. Set path and parameter details in model_config.yml

Note: If the training is interrupted and you need to resume training, you can set resume_ckpt and resume_D_ckpt.

## Run testing
```python

python test.py (main setting test_ckpt/test_path/test_mask_1_60_path/save_img_path)

```
1. set the config path ('./config/model_config.yml')
2. Set path and parameter details in model_config.yml

## Quantitative comparison

- Places365 

<img src="https://i.imgur.com/rU4n2cG.png" width="1312" height="350">

Quantitative evaluation of inpainting on Places365 dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception ´ Distance (FID) metrics. The ▲ denotes larger, and ▼ denotes lesser of the parameters compared to our proposed model. (Bold means the 1st best; Underline means the 2nd best; † means higher is better; ¶ means lower is better)

- CelebA 

<img src="https://i.imgur.com/hfnk1QZ.png" width="1312" height="350">

Quantitative evaluation of inpainting on CelebA dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance ´ (FID) metrics. The ▲ denotes larger, and ▼ denotes lesser of the parameters compared to our proposed model. (Bold means
the 1st best; Underline means the 2nd best; † means higher is better; ¶ means lower is better)

- FFHQ 

<img src="https://i.imgur.com/C1DTqt2.png" width="1312" height="350">

 Quantitative evaluation of inpainting on FFHQ dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance ´ (FID) metrics. (Bold means the 1st best; † means higher is better; ¶ means lower is better; S means 5% to 20% mask
range; M means 21% to 40% mask range; L means 41% to 60% mask range)

- Paris Street View 

<img src="https://i.imgur.com/MHf8WQX.png" width="1312" height="350">

Quantitative evaluation of inpainting on Paris Street View dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception ´ Distance (FID) metrics. (Bold means the 1st best; † means higher is better; ¶ means lower is better; S means 5% to
20% mask range; M means 21% to 40% mask range; L means 41% to 60% mask range)


All training and testing base on same 3090.

## Qualitative comparisons

- Places365

<img src="https://i.imgur.com/FMGm4mB.jpg" width="1000" style="zoom:100%;">

Qualitative results of Places365 dataset among all compared models. From left to right: Masked image, DeepFill_v2, HiFill, Iconv, AOT-GAN, HiFill, CRFill, TFill, and Ours. Zoom-in for details.

- CelebA

<img src="https://i.imgur.com/hPPQQ3W.jpg" width="1000" style="zoom:100%;">

Qualitative results of CelebA dataset among all compared models. From left to right: Masked image, RW, DeepFill_v2, Iconv, AOT-GAN, CRFill, TFill, and Ours. Zoom-in for details.

## Ablation study

- Transformer and HSV loss

<div align=center>
<img src="https://i.imgur.com/DoYLVKD.png" width="410" height="150"><img src="https://imgur.com/Utxgfzs.jpg" width="410" height="150">
</div>

(left) : Ablation study label of transformer and HSV experiment.

(right) : Ablation study of color deviation on inpainted images. From left to right: Masked images, w/o TotalHSV loss, and TotalHSV loss (w/o V).

## Object removal

<div align=center>
<img src="https://i.imgur.com/IYIMow7.jpg" width="1300" height="350">
</div>

Object removal (size 256×256) results. From left to right: Original image, mask, object removal result.


## Acknowledgement
This repository utilizes the codes of following impressive repositories   
- [ZITS](https://github.com/DQiaole/ZITS_inpainting)
- [LaMa](https://github.com/saic-mdal/lama)
- [CSWin Transformer](https://github.com/microsoft/CSWin-Transformer)
- [Vision Transformer](https://github.com/google-research/vision_transformer)

---
## Contact
If you have any question, feel free to contact wiwi61666166@gmail.com

## Cite
```
@article{liu2023lightweight,
  title={Lightweight Image Inpainting by Stripe Window Transformer with Joint Attention to CNN},
  author={Liu, Tsung-Jung and Chen, Po-Wei and Liu, Kuan-Hsien},
  journal={arXiv preprint arXiv:2301.00553},
  year={2023}
}

```

