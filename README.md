# TS-ESWT-Net: Two-Stage Efficient Stripe Window Transformer Networks for Image Inpainting

<img src="https://i.imgur.com/LBIfNME.png" alt="https://i.imgur.com/LBIfNME.png" title="https://i.imgur.com/LBIfNME.png" width="1312" height="350">

# Environment
- Python 3.7.0
- pytorch
- opencv
- PIL  
- colorama

or see the requirements.txt

# How to try

## Download dataset (Places365、CelebA、FFHQ、Paris Street View)
- [Places365](http://Places365.csail.mit.edu/)  
- [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)  
- [FFHQ](https://drive.google.com/drive/folders/1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP)
- [Paris Street View](https://github.com/pathak22/context-encoder/issues/24) (Non-publicise, please ask the paper author)


## Set dataset path

Edit txt/xxx.txt (set path in config)
```python

data_path = './txt/train_path.txt'
mask_path = './txt/train_mask_path.txt'
val_path = './txt/val_path.txt'
val_mask_path = './val_mask_file/' # path
test_path: './txt/test_path.txt'
test_mask_1_60_path: './test_mask_1+10_file/' # path

```

txt example
```python

E:/Places365/data_256/00000001.jpg
E:/Places365/data_256/00000002.jpg
E:/Places365/data_256/00000003.jpg
E:/Places365/data_256/00000004.jpg
E:/Places365/data_256/00000005.jpg
 ⋮

```

**You can refer to our example.txt in the txt path**

## Preprocessing  
In this implementation, masks are automatically generated by ourself. stroke masks mixed randomly to generate proportion from 1% to 60%.

strokes (from left to right 20%-30% 30%-40% 40%-50% 50%-60%)
<img src="https://imgur.com/m3CStkN.png" alt="https://imgur.com/m3CStkN.png" title="https://imgur.com/m3CStkN.png" width="1000" height="200">

## Run training
```python

python train.py (main setting data_path/mask_path/val_path/val_mask_path/batch_size/train_epoch)

```
1. set the config path ('./config/model_config.yml')
2. Set path and parameter details in model_config.yml

Note: If the training is interrupted and you need to resume training, you can set resume_ckpt and resume_D_ckpt.

## Run testing
```python

python test.py (main setting test_ckpt/test_path/test_mask_1_60_path/save_img_path)

```
1. set the config path ('./config/model_config.yml')
2. Set path and parameter details in model_config.yml

## Quantitative comparison

- Places365 

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/Places365.png" width="1312" height="350">

Quantitative evaluation of inpainting on Places365 dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception ´ Distance (FID) metrics. The ▲ denotes larger, and ▼ denotes lesser of the parameters compared to our proposed model. (Bold means the 1st best; Underline means the 2nd best; † means higher is better; ¶ means lower is better)

- CelebA 

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/CelebA.png" width="1312" height="350">

Quantitative evaluation of inpainting on CelebA dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance ´ (FID) metrics. The ▲ denotes larger, and ▼ denotes lesser of the parameters compared to our proposed model. (Bold means
the 1st best; Underline means the 2nd best; † means higher is better; ¶ means lower is better)

- FFHQ 

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/FFHQ.png" width="1312" height="200">

 Quantitative evaluation of inpainting on FFHQ dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance ´ (FID) metrics. (Bold means the 1st best; † means higher is better; ¶ means lower is better; S means 5% to 20% mask
range; M means 21% to 40% mask range; L means 41% to 60% mask range)

- Paris Street View 

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/Paris%20Street%20View.png" width="1312" height="200">

Quantitative evaluation of inpainting on Paris Street View dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception ´ Distance (FID) metrics. (Bold means the 1st best; † means higher is better; ¶ means lower is better; S means 5% to
20% mask range; M means 21% to 40% mask range; L means 41% to 60% mask range)

All training and testing base on same 3090.

## Qualitative comparisons

- Places365

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/Places365%20-1.png" width="1000" style="zoom:100%;">

Qualitative comparison of inpainting results on the Places365 dataset. From left to right: ground truth image, input image, and outputs from CA, DeepFill-V2, HiFill, Iconv, CRFill, RW, AOT-GAN, TFill, SWMHT-Net, MSCSWT-Net, FcF, SketchRefiner, and TS-ESWT-Net (Ours).

- CelebA

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/CelebA%20-1.png" width="1000" style="zoom:100%;">

Qualitative comparison of inpainting results on the CelebA dataset. From left to right: ground truth image, input image, and outputs from CA, DeepFill-V2, Iconv, RFR, CRFill, RW, AOT-GAN, TFill, SWMHT-Net, MSCSWT-Net, FcF, M2S, SketchRefiner, and TS-ESWT-Net (Ours).

- FFHQ

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/FFHQ%20-1.png" width="1000" style="zoom:100%;">

Qualitative comparison of inpainting results on the FFHQ dataset. From left to right: ground truth image, input image, and outputs from CA, TFill, SWMHT-Net, MSCSWT-Net, FcF, SketchRefiner, and TS-ESWT-Net (Ours).

- Paris Street View

<img src="https://github.com/bobo0303/ESWT-Net/blob/main/img/Paris%20Street%20View%20-1.png" width="1000" style="zoom:100%;">

Qualitative comparison of inpainting results on the Paris Street View dataset. From left to right: ground truth image, input image, and outputs from SN, RW, RFR, SWMHT-Net, MSCSWT-Net, FcF, SketchRefiner, and TS-ESWT-Net (Ours).

## Ablation study

- Ablation study table

<div align=center>
<img src="https://i.imgur.com/IjlLw3j.png" width="650" height="250">
</div>

Ablation study of all modual we used with size 256×256 images on Places365 dataset. We report Peak signal-to-noise ratio (PSNR), structural similarity (SSIM). (Bold means the 1st best; Underline means the 2nd best; † means higher is better; V means included module; V∗ means included module and get results from this stage.)

- Ablation study Qualitative comparisons

<div align=center>
<img src="https://i.imgur.com/UFeJK0D.png" width="550" height="300">
</div>

The results of each ablation experiment are shown. There are respective removed modules at the bottom of each image. Among them, TS-ESWT (coarse) represents the original model design but the coarse result of the first stage and TS-ESWT (refine) represents the output refinement result of the second stage of the original model.

## Object removal

<div align=center>
<img src="https://i.imgur.com/tKALlyh.png" width="650" height="250">
</div>

Object removal (size 256×256) results. From left to right: Original image, mask, object removal result.


## Acknowledgement
This repository utilizes the codes of following impressive repositories   
- [ZITS](https://github.com/DQiaole/ZITS_inpainting)
- [LaMa](https://github.com/saic-mdal/lama)
- [CSWin Transformer](https://github.com/microsoft/CSWin-Transformer)
- [Vision Transformer](https://github.com/google-research/vision_transformer)
- [SWMHT-Net](https://github.com/bobo0303/LIGHTWEIGHT-IMAGE-INPAINTING-BY-STRIPE-WINDOW-TRANSFORMER-WITH-JOINT-ATTENTION-TO-CNN)
- [MSCSWT-Net](https://github.com/bobo0303/MSCSWT-Net)

---
## Contact
If you have any question, feel free to contact wiwi61666166@gmail.com


